{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniProj_Q2_PartE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcJDApDcK0O9",
        "colab_type": "code",
        "outputId": "b38f44e5-d367-4d83-ff40-4224f5743b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/parhamzm/Beijing-Pollution-DataSet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Beijing-Pollution-DataSet' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-KcI9tFLGXP",
        "colab_type": "code",
        "outputId": "7d5aa983-c76c-41e3-8eb6-065397765ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls Beijing-Pollution-DataSet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pollution.csv  polution_dataSet.npy  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NuzKJEnLNyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_0pmTGAHWmf",
        "colab_type": "text"
      },
      "source": [
        "# **Data Pre Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjcwsJBc9eIG",
        "colab_type": "code",
        "outputId": "7dcb16f9-13d3-4746-8e9b-394e38e6c89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "DATA_DIR = \"Beijing-Pollution-DataSet/\"\n",
        "from pandas import read_csv\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "import random\n",
        "\n",
        "def select_random_rows(seq_start=0, seq_end=12000, rand_percent=20):\n",
        "    rand_num = int((20*(seq_end-seq_start))/100)\n",
        "    rand_gen = random.sample(range(seq_start, seq_end), rand_num)\n",
        "    return rand_gen\n",
        "\n",
        "def apply_nan(sequence, seq_start=0, seq_end=12000, rand_percent=20):\n",
        "    for i in range(0, 8):\n",
        "        rand_r = select_random_rows(seq_start=seq_start, seq_end=seq_end, rand_percent=rand_percent)\n",
        "        np.put(sequence[seq_start:seq_end, i], rand_r, np.nan)\n",
        "    return sequence\n",
        "\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps=11, n_samples=12000, start_from=0):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(start_from, (start_from + n_samples)):\n",
        "        # find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "        # check if we are beyond the dataset\n",
        "        # gather input and output parts of the pattern\n",
        "\t\tseq_x = sequences[i:end_ix, :]\n",
        "\t\tseq_y = sequences[end_ix, 0]\n",
        "\t\ty.append(seq_y)\n",
        "\t\tX.append(seq_x)\n",
        "  \n",
        "\treturn array(X), array(y)\n",
        "\n",
        "\n",
        "data = np.load(DATA_DIR + 'polution_dataSet.npy')\n",
        "scaled_data = data\n",
        "data1 = np.load(DATA_DIR + 'polution_dataSet.npy')\n",
        "\n",
        "# specify the number of lag hours\n",
        "n_hours = 11\n",
        "n_features = 8\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "n = apply_nan(data, seq_start=0, seq_end=15000, rand_percent=20)\n",
        "occurrences = np.count_nonzero(np.isnan(data1[:, 1]))\n",
        "print(occurrences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayZM_t0NOvKc",
        "colab_type": "code",
        "outputId": "6f3df24c-34c0-4b4f-9877-c928dba8d1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example of evaluating a model after an imputer transform\n",
        "from numpy import nan\n",
        "from pandas import read_csv\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import numpy.ma as ma\n",
        "data = np.where(np.isnan(data), ma.array(data, mask=np.isnan(data)).mean(axis=0), data)\n",
        "\n",
        "\n",
        "occurrences = np.count_nonzero(np.isnan(data1[:, :]))\n",
        "print(\"Number of Nan in Dataset : => \", occurrences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Nan in Dataset : =>  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aJIvwX7PQAX",
        "colab_type": "code",
        "outputId": "0e03e1a7-9d60-4be0-df83-214a0456d4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "loss1 = criterion(torch.tensor(data1[:, 0], dtype=torch.float32), torch.tensor(data[:, 0], dtype=torch.float32))\n",
        "loss2 = criterion(torch.tensor(data1[:, 1], dtype=torch.float32), torch.tensor(data[:, 1], dtype=torch.float32))\n",
        "loss3 = criterion(torch.tensor(data1[:, 2], dtype=torch.float32), torch.tensor(data[:, 2], dtype=torch.float32))\n",
        "loss4 = criterion(torch.tensor(data1[:, 3], dtype=torch.float32), torch.tensor(data[:, 3], dtype=torch.float32))\n",
        "loss5 = criterion(torch.tensor(data1[:, 4], dtype=torch.float32), torch.tensor(data[:, 4], dtype=torch.float32))\n",
        "loss6 = criterion(torch.tensor(data1[:, 5], dtype=torch.float32), torch.tensor(data[:, 5], dtype=torch.float32))\n",
        "loss7 = criterion(torch.tensor(data1[:, 6], dtype=torch.float32), torch.tensor(data[:, 6], dtype=torch.float32))\n",
        "loss8 = criterion(torch.tensor(data1[:, 7], dtype=torch.float32), torch.tensor(data[:, 7], dtype=torch.float32))\n",
        "print(\"MSE Loss Column 1 :=> {:.5f}\".format(loss1.item()))\n",
        "print(\"MSE Loss Column 2 :=> {:.5f}\".format(loss2.item()))\n",
        "print(\"MSE Loss Column 3 :=> {:.5f}\".format(loss3.item()))\n",
        "print(\"MSE Loss Column 4 :=> {:.5f}\".format(loss4.item()))\n",
        "print(\"MSE Loss Column 5 :=> {:.5f}\".format(loss5.item()))\n",
        "print(\"MSE Loss Column 6 :=> {:.5f}\".format(loss6.item()))\n",
        "print(\"MSE Loss Column 7 :=> {:.5f}\".format(loss7.item()))\n",
        "print(\"MSE Loss Column 8 :=> {:.5f}\".format(loss8.item()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE Loss Column 1 :=> 0.00050\n",
            "MSE Loss Column 2 :=> 0.00338\n",
            "MSE Loss Column 3 :=> 0.00291\n",
            "MSE Loss Column 4 :=> 0.00253\n",
            "MSE Loss Column 5 :=> 0.00639\n",
            "MSE Loss Column 6 :=> 0.00073\n",
            "MSE Loss Column 7 :=> 0.00005\n",
            "MSE Loss Column 8 :=> 0.00014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsDIxzPPGVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}